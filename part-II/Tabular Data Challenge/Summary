1.	Code to build the model is present here.
2.	The prediction.csv file is attached in this repository.
3.	From the evaluation metrics standpoint seems like random forest model performed better as a base model and can be further tuned to improved batter. It was compared against linear regression, linear regression with normalized data, logistic regression with normalized data, random forest with normalized data
4.	“Is_candidate” dependent variable was unbalanced, but the imbalance was not too high. Generally, an imbalance is said to be present when a classifier has 99:1 ratio or 98:2 ratio. So, I didn’t address imbalance, instead I used tree-based models, in my personal experience bagging models generally perform better with imbalance and I checked its performance against a logistic regression classifier which are more cost sensitive. To further tune my results I definitely would use under sampling. If I had to perform sampling of any sort for this case, I would have gone for random under sampling of majority class, SMOTE method can increase noise in the data, oversampling can increase biases since we duplicate the minority class. 
5.	For real time predictions, random forest, its prediction time was the fastest and had better balanced accuracy, sensitivity, F1 Score and least training time. The results were close or consistent on both test and validation data set
